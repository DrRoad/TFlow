<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Traffic Flow</title>

		<meta name="description" content="An automatic traffic flow parameter estimation based on computer vision">
		<meta name="author" content="Eduardo Henrique Arnold">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>TFlow</h1>
					<h4>An automatic traffic flow parameter estimation based on computer vision</h4>
					<p>
						<small>Author <br> <a href="https://github.com/eduardohenriquearnold">Eduardo Henrique Arnold</a></small>
					</p>
				</section>

				<section>
					<h2>Motivation</h2>
                                        <ul>
                                                <li class="fragment">Modern society requires efficient traffic measurement and control.</li>
                                                <li class="fragment">Growing cities need car use statistics to plan infrastructure upgrades.</li>
                                                <li class="fragment">Efficient control can be achieved when using real time data. </li>
                                                <li class="fragment">A computer vision approach have lower installation costs and higher flexibility compared to induction loops.</li>
                                                <li class="fragment">Publicly available video streams make this solution attractive.</li>
                                        </ul>
                                        
				</section>
				
				<section>
				        <h2>Objectives</h2>
				        <ul>
				                <li>Detect and possibly track vehicles.</li>
				                <li>Obtain traffic parameters such as flow and occupancy for a given road.</li>
				                <li>The above mentioned measures allow an estimative of the lanes condition (busy, traffic jam, low traffic).</li>
				        </ul>
				        
				</section>
				
				<section>
				        <h2>Problem Definition</h2>
                                        
                                        <p>Given a video stream it is necessary to find all vehicles in a frame. Next, to obtain the cars parameters such as area and velocity.</p>
				</section>
				
				
				<!-- Algorithm -->
			        <section>
			                <h2>Propposed solution</h2>				        
		                </section>	
			                
                                <!-- ROI -->			                
                                <section>
			                <section>
			                        <h4>1. Define and extract a Region of Interest (ROI)</h4>

				                <ul>
				                        <li>A frame might have a wide view, we need to focus the road plane only.</li>
				                        <li>Thus, we select a set of appropriate points and perform a <b>perspective transform</b></li>				                       
				                </ul> 			                        
			                </section>	
			                <section>				                
			                        <img class="stretch" width="500" height="250" data-src="imgs/f1.jpg">
			                        <h6>Original frame</h6>
			                        <small>This image was obtained from a publicly available webcam at Osmar Cunha Street, Florian√≥polis centre.</small>
			                </section>
			                <section>				                
			                        <img class="stretch" width="500" height="250" data-src="imgs/roi1.jpg">
			                        <h6>ROI obtained using perspective transform</h6>
			                </section>
		                </section>
		                
		                <!-- BS -->
		                <section>
		                        <section>
		                                <h4>2. Detect cars</h4>
		                                <p>It is necessary to detect cars in the image in order to track and count them. How should this be done?
		                                </p>

                                                <p class="fragment">Using a classifier?</p>
                                                <p class="fragment">Or better, using a Background Subtraction method that does not need any previous information about the system. </p>
		                        </section>
		                        <section>
			                        <h4>2.a Detect moving regions</h4>
                                                <h5>Background Subtraction (BS)</h5>
				                <ul>
				                        <li>A background model is generated and a foreground mask is obtained by subtracting the original frame with the estimated background</li>
				                        <li>The background model is updated at every frame, reducing the effect of illumination changes over time.</li>
			                                <li>The model used in this case uses a Mixture of Gaussians (MOG) representation for each pixel, trying to represent the most often occuring values.</li>
			                                <li>Careful adjustment of the BS object can give better results, however may be scene depedent.</li>
				                        				                       
				                </ul> 	
			                </section>		                        
			                <section>				                
			                        <img class="stretch" width="600" height="400" data-src="imgs/bs.jpg">
			                        <small>Result obtained with Background Subtraction</small>
			                </section>	
			                <section>
                                                <h5>2.b Background Subtraction (BS)</h5>
                                                
                                                <p>This method gives a great result and does not require training. However, it introduces another problem.</p>
                                                <p class="fragment">What happens when a car stay still for a long period of time?</p>
			                        <img class="fragment stretch" width="500" height="500" data-src="imgs/bs-problem.jpg">
                                                <p class="fragment">To account for this problem, we will maintain a record of all detected cars and check if they are still in the scene using a descriptor described in later section.</p>
                                        </section>		                
		                </section>
		                
		                <!-- Detect Foreground Cars -->
		                <section>
		                        <section>
		                                <h4>3. Detect cars in foreground mask</h4>
		                                
		                                <ol>
		                                        <li>Get contours of foreground mask</li>
		                                        <li>Get bounding boxes around each of them</li>
		                                        <li>Filter boxes by area to remove noise and false positives (area less than 1% of ROI)</li>
		                                        <li>Create a <i>Car</i> object containing the region dimensions, time of detection and feature extracted with a descriptor from the current frame</li>
	                                                <li>Store all detected cars in a <i>"foregroundCars"</i> vector.</li>
		                                </ol>
		                        </section>
		                        		                        
		                        <section>
                                                <img class="stretch" width="500" height="500" data-src="imgs/fg-detected.jpg">
                                                <h6>Bounding box found around a valid region.</h6>
		                        </section>
		                        
		                        <!-- Descriptor -->
		                        <section>
		                                <h4>Descriptor</h4>
		                                
		                                <ul>
		                                        <li>A descriptor tries to describe useful, <b>discriminating</b>, information about a set of data</li>
		                                        <li>It is necessary to indentify and distinguish the objects or data we are handling</li>
		                                        
		                                        <li>It oftens convert a great amount of data into a significant discriminating amount of useful information</li>
		                                        		                                        		                                       
		                                </ul>
		                                
		                                <div class="fragment">
		                                        <p>In this project two descriptors were used:</p>
		                                        <ol>
		                                                <li>Raw pixels of the corresponding car patch</li>
		                                                <li>Normalized color histogram of the corresponding car patch</li>
		                                        </ol>
		                                        <p>Their performance will be compared later.</p>
		                                </p>
		                        </section>
		                </section>
		                
		                <!-- Car Update process -->
		                <section>
		                        <section>
		                                <h4>4. Car Update Process</h4>
		                                <ul>
		                                        <li>We assume a vector of known moving cars, initially empty.</li>
		                                        <li>Because moving cars will be repeatedly detected, we need to match these new detected cars to the movingCars vector, updating the movingCars vector if there's a match</li>
		                                        
		                                        <li>Also, we need to check if cars that did not move are still in the scene, by comparing the current frame feature with the one extracted when it was last detected. </li>
		                                        
		                                        <li>Finally, we need to add the remaining detected cars that did not match to the moving cars vector, to account for new moving cars.</li>
		                                </ul>
		                        </section>
		                        
		                        <section>
		                                <h4>4.a Matching Process</h4>
		                                <p>A pair of cars is considered a match (correspond to the same car, but shifted in time) if the following conditions are met:</p>
		                                
		                                <ol>
		                                        <li>Distance between their centroids is smaller than a threshold $t_{dist}$ </li>
		                                        <li>Ratio of their areas meets the condition $1+t_a \leq \frac{A_{c1}}{A_{c2}} \leq 1-t_a$ where $t_a$ is a area threshold deviation. </li>
		                                        <li>Correlation calculated between their features is greater than a threshold $t_f$</li>
		                                </ol>		                                
		                        </section>
		                        
		                        <section>
		                                <h4>4.a Matching Process</h4>
		                                
		                                <p>If there is a match, the following actions are executed:</p>
		                                <ul>
		                                        <li>Car position, feature and last detected time is updated</li>
		                                        <li>Car relative velocity is calculated using $v=\frac{d}{\Delta t}$</li>
		                                </ul>
		                        </section>
		                        
		                        <section> 
		                                <h4>4.b Car Validation Process</h4>
		                                
		                                <p>Cars in the vector that did not get updated need validation, to make sure they are still present in the scene. </p>
		                                
		                                <ol>
		                                        <li>The descriptor is used to get a new feature from the patch of the current frame where the car was last found.</li>
		                                        <li>The correlation between this new extracted feature and the one previous recorded is computed.</li>
		                                        <li>The car is considered to still be in the scene if the correlation value is higher than a given threshold $t_\text{corr scene}$</li>
		                                        
		                                </ol>
		                        </section>
		                        
		                       <section>
		                                <h4>4.c Car update process conclusion</h4>
		                                
		                                <p>By adopting all these measures, it is possible to know the position and velocity of any given car in an instant by inspection of the <i>moving cars</i> vector. <p>
		                                
		                                <p>This approach also solves the problem of car "fading" introduced by the use of the background subtractor method.</p>
		                       </section>		                        
	                        </section>
	                        
	                        <!-- Flow information -->
	                        
	                        
	                        
	                
	                </section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom
				
				 math: {
                                        mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
                                        config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
                                    },

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
                                        { src: 'plugin/math/math.js', async: true }
					
				]
			});

		</script>

	</body>
</html>
